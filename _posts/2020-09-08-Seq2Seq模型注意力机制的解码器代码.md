---
layout:       post
title:        "Seq2Seq模型：基于注意力机制的解码器"
subtitle:     "Seq2Seq模型：基于注意力机制的解码器"
date:         2020-09-08 17:13:00
author:       "xuepro"
header-img:   "img/home_bg.jpg"
header-mask:  0.3
catalog:      true
multilingual: true
tags:
    - DL
---



### 基于嵌入层的编码器

```python
from rnn import *
from Layers import *
from train import *

class EncoderRNN_Embed(object):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size,self.hidden_size = input_size,hidden_size   
        #print(input_size, hidden_size)
        self.embedding = Embedding(input_size, hidden_size)
        self.gru = GRU(hidden_size, hidden_size,1)

    def forward(self, input, hidden):
        #(1,1,input_size)
        #input = vecterization(input_verb,input)
        #output, hidden = self.gru(input, hidden)
        
        self.embedded_x = []
        self.embedded_out = []
        embed_out = []
        for x in input:              
            embedded = self.embedding(x).reshape(1,1,-1)
            self.embedded_x.append(self.embedding.x)
            self.embedded_out.append( embedded)
            
        self.embedded_out = np.concatenate(self.embedded_out,axis=0) 
        output, hidden = self.gru(self.embedded_out, hidden)
        return output, hidden

    def __call__(self,input, hidden):
        return self.forward(input, hidden)
    
    def initHidden(self):
        return np.zeros((1, 1, self.hidden_size))
    
    def parameters(self):
        return self.gru.parameters()
 
    def backward(self,dhs): 
        dinput,dhidden = self.gru.backward(dhs,self.embedded_out)
        T = dinput.shape[0]
        for t in range(T):
            dinput_t = dinput[t]
            self.embedding.x = self.embedded_x[t]  ## recover the original x when do forward
            #self.embedding.backward(d_embeded)     
            self.embedding.backward(dinput_t)
```



### 基于注意力机制的解码器

按照pytorch教程的图从0编写的编码器

![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)

```python
from Layers import *
from rnn import *
import util
    
class  DecoderRNN_Atten(object):
    def __init__(self, hidden_size, output_size,num_layers=1,teacher_forcing_ratio = 0.5,dropout_p=0.1, \
                 max_length=MAX_LENGTH):
        super(DecoderRNN_Atten, self).__init__()
       
        self.hidden_size = hidden_size       
        self.num_layers = 1
        self.teacher_forcing_ratio = teacher_forcing_ratio
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = Embedding(output_size, hidden_size)
        self.dropout = Dropout(self.dropout_p)
                
        self.attn = Dense(self.hidden_size * 2, self.max_length)
        #self.attn = Atten(hidden_size)    
        self.attn_combine = Dense(self.hidden_size * 2, self.hidden_size)        
        self.relu = Relu()
        
        self.gru = GRU(hidden_size, hidden_size,1)
        self.out = Dense(hidden_size, output_size)
        
        self.layers = [self.embedding,self.attn,self.attn_combine,self.gru,self.out]
        self._params = None
        self.use_dropout = False

    def initHidden(self,batch_size):
        # This is what we'll initialise our hidden state as
        self.h_0 =  np.zeros((self.num_layers, batch_size, self.hidden_size))    
    
    def forward_step(self, input, prev_hidden,encoder_outputs,training=True):
        embedded = self.embedding(input)  #(B,D))
        self.embedded_x.append(self.embedding.x) 
        
        if self.use_dropout:
            embedded = self.dropout(embedded,training)
            self.dropout_mask.append(self.dropout._mask)        
             
        attn_out = self.attn(np.concatenate((embedded, prev_hidden[0]),axis=1))
        self.attn_x.append(self.attn.x)
        #context,alphas,energies = self.attn(hidden, encoder_outputs)  
        #self.attn_x.append((context,alphas,energies,hidden))               
        
        attn_weights  = util.softmax(attn_out)        
        self.attn_weights_seq.append(attn_weights)
        
        attn_applied  = bmm(attn_weights,encoder_outputs)
        
        attn_combine_out = self.attn_combine(np.concatenate((embedded, attn_applied),axis=1))
        self.attn_combine_x.append(self.attn_combine.x)
          
        output = self.relu(attn_combine_out)   
        self.relu_x.append(self.relu.x)  
        
        relu_out = output.reshape(1,output.shape[0],-1)             
        self.gru_x.append(relu_out)  #output)  # input of gru          
        output_hs, hidden = self.gru(relu_out,prev_hidden) 
        self.gru_hs.append(self.gru.hs) #保持中间层的隐状态
        self.gru_zs.append(self.gru.zs) #保持中间层的计算结果 
            
        output_hs_t = output_hs[0]#seq_len = 1
        output = self.out(output_hs_t) 
        self.out_x.append(self.out.x)
        return output,hidden,output_hs_t
        
    def forward(self,input_tensor,encoder_outputs): #hidden,encoder_outputs): 
        self.encoder_outputs = encoder_outputs  #(T,B,D)
        self.attn_weights_seq = []
        target_length = input_tensor.shape[0] #nput_tensor.size(0)
        
        teacher_forcing_ratio = self.teacher_forcing_ratio
        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False
        
        hidden_t = encoder_outputs[-1].reshape(1,encoder_outputs[-1].shape[0],encoder_outputs[-1].shape[1])
        h_0 = hidden_t.copy()
        input_t = np.array([SOS_token])
           
        output_hs = []
        output = []  
        self.gru_x = [] #gru input       
        self.gru_hs = []
        self.gru_zs = []
        self.dropout_mask = []
        self.embedded_x = []
        self.relu_x = []
        self.attn_x = []
        self.attn_combine_x = []
        self.attn_weights_seq = []
        self.out_x = []
        
        encoder_outputs = np.pad(self.encoder_outputs,((0,self.max_length-self.encoder_outputs.shape[0]),(0,0),(0,0)), 'constant')
        for t in range(target_length):
            output_t, hidden_t,output_hs_t = self.forward_step(input_t, hidden_t,encoder_outputs)
            output_hs.append(output_hs_t)
            output.append(output_t)
           
            if use_teacher_forcing:
                input_t = input_tensor[t]  # Teacher forcing
            else:             
                input_t = np.argmax(output_t)  #最大概率
                if input_t== EOS_token:
                    break  
                input_t = np.array([input_t])
                
        output = np.array(output)
        self.output_hs = np.array(output_hs)
        self.h_0 = h_0  
        return  output
    
    def __call__(self, input, hidden):
        return self.forward(input, hidden)
    
    def evaluate(self, encoder_outputs,max_length):
        hidden = encoder_outputs[-1]
        hidden = hidden.reshape(1,hidden.shape[0],hidden.shape[1])
        input_T = self.encoder_outputs.shape[0]
        encoder_outputs = np.pad( self.encoder_outputs,((0,self.max_length- input_T),(0,0),(0,0)), 'constant')
        
        # input:(1, batch_size=1, input_size)  
        input = np.array([SOS_token])
        decoded_words = []
        for t in range(max_length):
            output,hidden,_ = self.forward_step(input,hidden,encoder_outputs,False)
            output = np.argmax(output)         
            if output==EOS_token:
                break;
            else:           
                decoded_words.append(output)               
                input = np.array([output])
        return    decoded_words      
    
    def backward(self,dZs):
        #input = np.concatenate(self.input,axis=0)
        input_T = self.encoder_outputs.shape[0]
        d_encoder_outputs  = np.zeros_like(self.encoder_outputs)
        T = len(dZs)
        encoder_outputs = np.pad( self.encoder_outputs,((0,self.max_length- input_T),(0,0),(0,0)), 'constant')
        
        for i in reversed(range(T)):
            self.out.x = self.out_x[i]
            dh = self.out.backward(dZs[i])            
            
            dhs = np.expand_dims(dh, axis=0)
            self.gru.hs = self.gru_hs[i]
            self.gru.zs = self.gru_zs[i]
            drelu_out,dprev_hidden = self.gru.backward(dhs,self.gru_x[i])           
            drelu_out = drelu_out.reshape(drelu_out.shape[1],drelu_out.shape[2])         
            
            self.relu.x = self.relu_x[i]
            d_relu_x = self.relu.backward(drelu_out)
            d_attn_combine_out = d_relu_x
            
            self.attn_combine.x = self.attn_combine_x[i]        
            d_attn_combine_x = self.attn_combine.backward(d_attn_combine_out)
            d_embedded, d_attn_applied = d_attn_combine_x[:,:self.hidden_size], d_attn_combine_x[:,self.hidden_size:]
            
            attn_weights = self.attn_weights_seq[i]
            d_attn_weights,d_encoder_outputs_t = bmm_backward(d_attn_applied,attn_weights,encoder_outputs)
        
            d_attn_out  = softmax_backward_2(attn_weights,d_attn_weights)
            
            self.attn.x = self.attn_x[i]
            d_attn_x = self.attn.backward(d_attn_out)
            
            d_embedded_2, dprev_hidden_2 = d_attn_x[:,:self.hidden_size], d_attn_x[:,self.hidden_size:]
            
            d_embedded+= d_embedded_2
            
            if self.use_dropout:
                self.dropout._mask = self.dropout_mask[i]
                d_embedding = self.dropout.backward(d_embedded)                  
            else:
                d_embedding = d_embedded             
            
            self.embedding.x = self.embedded_x[i]  ## recover the original x when do forward
            self.embedding.backward(d_embedding)
            
            dprev_hidden+= dprev_hidden_2            
            d_encoder_outputs +=d_encoder_outputs_t[:input_T] #每个时刻的都要累加        
        
        d_encoder_outputs[input_T-1]+=dprev_hidden[0]       
        return dprev_hidden,d_encoder_outputs #dhidden 
  
    def parameters(self):
        if self._params is None:
            self._params = []
            for layer in self.layers:
                for  i, _ in enumerate(layer.params):  
                    self._params.append([layer.params[i],layer.grads[i]])  
        return self._params     
```

单词表：
```python
import numpy as np
from collections import defaultdict
SOS_token = 0
EOS_token = 1
UNK_token = 2
    
class Vocab:
    def __init__(self,min_count=1,corpus = None): 
        self.min_count = 1
        self.word2count = {}
        self.word2index = {"SOS":0,"EOS":1, "UNK":2}        
        self.index2word = {0: "SOS", 1: "EOS",2: "UNK"}
        self.n_words = 3  # Count SOS and EOS
        if corpus is not None:
            for sentence in corpus:
                self.addSentence(sentence)
            self.build()               

    def addSentence(self, sentence):
        if isinstance(sentence,str):
            for word in sentence.split(' '):
                self.addWord(word)                
        else:
            for word in sentence:
                self.addWord(word)              

    def addWord(self, word):
        if word not in self.word2count:
            self.word2count[word] = 1
        else:
            self.word2count[word] += 1
            
    def build(self):
        for word in self.word2count:
            if self.word2count[word]<self.min_count:
                self.word2index[word] = UNK_token
            else:
                self.word2index[word] = self.n_words 
                self.index2word[self.n_words] = word
                self.n_words += 1
                
            
vocab = Vocab()
vocab.addSentence("i am from china")
vocab.build()

print(vocab.word2index["i"])
print(vocab.index2word[4])
```
 
